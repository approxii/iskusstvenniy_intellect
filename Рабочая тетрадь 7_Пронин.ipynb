{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b08f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5482733885555816\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.1, 0.1, 0.1])\n",
    "        bias = 0\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.h3 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([1, 2, 3])\n",
    "print(network.feedforward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ad651bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9999998874648361, 0.9999998874648361)\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        offset = 15\n",
    "        self.h1 = Neuron(weights, offset)\n",
    "        self.h2 = Neuron(weights, offset)\n",
    "        self.o1 = Neuron(weights, offset)\n",
    "        self.o2 = Neuron(weights, offset)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self. h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1, out_o2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, offset):\n",
    "        self.weights = weights\n",
    "        self.offset = offset\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.offset\n",
    "        return sigmoid(total)\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab8da17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7200000000000002\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return tanh(x)\n",
    "\n",
    "def ReLU(x):\n",
    "    return max(0, x)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TanhNeuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return tanh(total)\n",
    "\n",
    "class TanhOurNeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.2, 0.2, 0.2])\n",
    "        bias = 0\n",
    "\n",
    "        self.h1 = TanhNeuron(weights, bias)\n",
    "        self.h2 = TanhNeuron(weights, bias)\n",
    "        self.h3 = TanhNeuron(weights, bias)\n",
    "        self.o1 = TanhNeuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "class ReLUNeutron:\n",
    "     def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "     def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return ReLU(total)\n",
    "\n",
    "class ReLuOurNeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.2, 0.2, 0.2])\n",
    "        bias = 0\n",
    "\n",
    "        self.h1 = ReLUNeutron(weights, bias)\n",
    "        self.h2 = ReLUNeutron(weights, bias)\n",
    "        self.h3 = ReLUNeutron(weights, bias)\n",
    "        self.o1 = ReLUNeutron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "network = ReLuOurNeuralNetwork()\n",
    "x = np.array([1, 2, 3])\n",
    "print(network.feedforward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e58d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sizes :  (150, 4) (150,)\n",
      "[1 0 2 2 0 0 2 2 2 0 0 1 2 1 2]\n",
      "[1 0 2 2 0 0 2 1 2 0 0 1 2 1 2]\n",
      "Test Accuracy : 0.933\n",
      "Training Accuracy : 0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\approxii\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\approxii\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.06623286e-01  5.50073550e+04]\n",
      " [-1.42685160e+00  1.23132754e+05]\n",
      " [-7.62282224e-01  5.72266289e+04]\n",
      " [-1.40256041e+00  1.18178549e+05]\n",
      " [-8.78237279e-01  6.51099626e+04]\n",
      " [-1.32219779e+00  1.13800168e+05]]\n",
      "[[3.20000e+00 5.44450e+04]\n",
      " [1.05000e+01 1.21872e+05]\n",
      " [2.90000e+00 5.66420e+04]\n",
      " [9.50000e+00 1.16969e+05]\n",
      " [3.20000e+00 6.44450e+04]\n",
      " [9.60000e+00 1.12635e+05]]\n",
      "Test R*2 Score : -2.070\n",
      "Training R°2 Score : -2.363\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "from sklearn.datasets import load_digits, load_boston, load_iris\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = load_digits()\n",
    "iris = load_iris()\n",
    "\n",
    "X_digits, Y_digits = iris.data, iris.target\n",
    "print('Dataset Sizes : ', X_digits.shape, Y_digits.shape)\n",
    "boston = load_boston()\n",
    "X_boston, Y_boston = boston.data, boston.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits, train_size=0.80, test_size = 0.20, stratify=Y_digits, random_state = 123)\n",
    "mlp_classifier = MLPClassifier(random_state=123)\n",
    "mlp_classifier.fit(X_train, Y_train)\n",
    "Y_preds = mlp_classifier.predict(X_test)\n",
    "print(Y_preds[:15])\n",
    "print(Y_test[:15])\n",
    "print('Test Accuracy : %.3f'%mlp_classifier.score(X_test, Y_test))\n",
    "print('Training Accuracy : %.3f'%mlp_classifier.score(X_train, Y_train))\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv'\n",
    "dataset = pd.read_csv(url)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.iloc[:, :].values, dataset.iloc[:, :].values,\n",
    "                                                    train_size=0.80, test_size=0.20, random_state=123)\n",
    "mlp_regressor = MLPRegressor(random_state=123)\n",
    "mlp_regressor.fit(X_train, Y_train)\n",
    "Y_preds = mlp_regressor.predict(X_test)\n",
    "print(Y_preds[:10])\n",
    "print(Y_test[:10])\n",
    "print('Test R*2 Score : %.3f' % mlp_regressor.score(X_test, Y_test))\n",
    "print('Training R°2 Score : %.3f' % mlp_regressor.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944481e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
